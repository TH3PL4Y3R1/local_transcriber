{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "778bf537",
   "metadata": {},
   "source": [
    "# Debugging GPU LLM Loading and Summarization with GGUF Models\n",
    "\n",
    "This notebook helps you debug GPU loading issues for GGUF models using llama-cpp-python, and demonstrates summarizing a .txt file using a custom summarize function. The workflow is organized in a 'debugging' folder with 'input', 'output', and 'src' subfolders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6274b6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 1: Setup Debugging Folder Structure\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "base_dir = Path.cwd() / 'debugging'\n",
    "input_dir = base_dir / 'input'\n",
    "output_dir = base_dir / 'output'\n",
    "src_dir = base_dir / 'src'\n",
    "\n",
    "for d in [input_dir, output_dir, src_dir]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Created folders:\\n- {input_dir}\\n- {output_dir}\\n- {src_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3a1bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 2: Import Required Libraries\n",
    "import os\n",
    "from pathlib import Path\n",
    "try:\n",
    "    from llama_cpp import Llama\n",
    "except ImportError:\n",
    "    print(\"llama_cpp not installed. Please install with the correct CUDA wheel.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45d7381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 3: Summarize Function Implementation (src/summarize.py)\n",
    "def summarize_text(text, model_path, llm_model, n_ctx=4096, n_gpu_layers=48):\n",
    "    \"\"\"\n",
    "    Summarize text using a GGUF LLM model with llama-cpp-python.\n",
    "    Args:\n",
    "        text (str): The input text to summarize.\n",
    "        model_path (str): Path to the directory containing the model.\n",
    "        llm_model (str): Filename of the GGUF model.\n",
    "        n_ctx (int): Context window size.\n",
    "        n_gpu_layers (int): Number of layers to run on GPU.\n",
    "    Returns:\n",
    "        str: The summary or error message.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    try:\n",
    "        from llama_cpp import Llama\n",
    "        model_file = os.path.join(model_path, llm_model)\n",
    "        llm = Llama(model_path=model_file, n_ctx=n_ctx, n_gpu_layers=n_gpu_layers)\n",
    "        prompt = f\"Resumí el siguiente texto en español de forma concisa y clara, usando viñetas si es posible.\\n\\nTexto:\\n{text[:2000]}\\n\\nResumen:\"\n",
    "        output = llm(prompt, max_tokens=256)\n",
    "        result = output['choices'][0]['text'].strip()\n",
    "        if not result:\n",
    "            print('[DEBUG] LLM returned empty summary. Full output:', output)\n",
    "            return '[LLAMA ERROR] LLM returned empty summary.'\n",
    "        print('[DEBUG] LLM summary:', result)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f'[DEBUG] Exception in LLM summarization: {e}')\n",
    "        return f\"[LLAMA ERROR] {e}\\n{text[:200]}...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e12fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 4: Load and Display Input Text File\n",
    "input_files = list(input_dir.glob('*.txt'))\n",
    "if not input_files:\n",
    "    print(f\"No .txt files found in {input_dir}. Please add a file to summarize.\")\n",
    "else:\n",
    "    input_file = input_files[0]\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        input_text = f.read()\n",
    "    print(f\"Loaded file: {input_file.name}\\n---\\n{input_text[:500]}\\n{'...' if len(input_text) > 500 else ''}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6b276b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 5: Run Summarization with GGUF Model\n",
    "# Set your GGUF model filename and model path here\n",
    "llm_model = 'your_model.gguf'  # Change to your actual model filename\n",
    "model_path = str(Path('models').absolute())  # Adjust if your models are elsewhere\n",
    "\n",
    "if not input_files:\n",
    "    summary = None\n",
    "    print(\"No input file to summarize.\")\n",
    "else:\n",
    "    summary = summarize_text(input_text, model_path, llm_model, n_ctx=4096, n_gpu_layers=48)\n",
    "    print(f\"Summary:\\n{summary}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fabbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 6: Save Summary to Output Folder\n",
    "if summary:\n",
    "    output_file = output_dir / f\"summary_{input_file.stem}.txt\"\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(summary)\n",
    "    print(f\"Summary saved to: {output_file}\")\n",
    "else:\n",
    "    print(\"No summary to save.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d61a446",
   "metadata": {},
   "source": [
    "# Troubleshooting and Checklist\n",
    "\n",
    "**If you encounter issues with GPU usage or summarization, review the following:**\n",
    "\n",
    "- Ensure you have an NVIDIA GPU and the correct CUDA Toolkit installed.\n",
    "- Install llama-cpp-python with the CUDA-enabled wheel for your CUDA version.\n",
    "- Set `n_gpu_layers` to a value that fits your VRAM (e.g., 40–50 for 8GB with a 7B model).\n",
    "- Watch the logs for lines like `llama.cpp: loading model on GPU: ... layers`.\n",
    "- If you see only CPU messages, double-check your CUDA version and llama-cpp-python installation.\n",
    "- For DLL errors, ensure CUDA's `bin` directory is in your PATH and reboot if needed.\n",
    "- For more help, see: https://github.com/abetlen/llama-cpp-python/discussions/1587\n",
    "\n",
    "## Checklist\n",
    "- [ ] NVIDIA GPU detected (`nvidia-smi`)\n",
    "- [ ] CUDA Toolkit installed and in PATH\n",
    "- [ ] Correct llama-cpp-python CUDA wheel installed\n",
    "- [ ] `n_gpu_layers` set in code\n",
    "- [ ] Logs show \"loading model on GPU\"\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
